# -*- coding: utf-8 -*-
"""CatsAndDogs_CNN_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10SvA2hlezQfqFEqHqvzheVcGZdNAH-TR
"""

from os import makedirs
from os import listdir
from shutil import copyfile
from random import seed
from random import random
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dropout

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

# organize dataset into a useful structure

# create directories
#'/gdrive/MyDrive/5644_ML_Project', 'Dataset_SkinCancer/train'
dataset_home = '/gdrive/MyDrive/5644_ML_Project/dataset_dogs_vs_cats/'
subdirs = ['train/', 'test/']
for subdir in subdirs:
	# create label subdirectories
	labeldirs = ['dogs/', 'cats/']
	for labldir in labeldirs:
		newdir = dataset_home + subdir + labldir
		makedirs(newdir, exist_ok=True)
# seed random number generator
seed(1)
# define ratio of pictures to use for validation
val_ratio = 0.25
# copy training dataset images into subdirectories
src_directory = '/gdrive/MyDrive/5644_ML_Project/dataset_dogs_vs_cats/'
for file in listdir(src_directory):
	src = src_directory + '/' + file
	dst_dir = 'train/'
	if random() < val_ratio:
		dst_dir = 'test/'
	if file.startswith('cat'):
		dst = dataset_home + dst_dir + 'cats/'  + file
		copyfile(src, dst)
	elif file.startswith('dog'):
		dst = dataset_home + dst_dir + 'dogs/'  + file
		copyfile(src, dst)

# to get data
train_path = '/gdrive/MyDrive/5644_ML_Project/dataset_dogs_vs_cats/train/'
print(train_path)
test_path = '/gdrive/MyDrive/5644_ML_Project/dataset_dogs_vs_cats/test/'
print(test_path)

train_datagen = ImageDataGenerator(rescale=1./255,validation_split=0.2) #,horizontal_flip=True)#)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        train_path,
        target_size=(32,32),
        batch_size=20,
        class_mode='binary',
        subset='training')
validation_generator = train_datagen.flow_from_directory(
        train_path,
        target_size=(32,32),
        batch_size=10,
        class_mode='binary',
        subset='validation')
test_generator = test_datagen.flow_from_directory(
        test_path,
        target_size=(32,32),
        batch_size=10,
        class_mode='binary')

# CNN model

#def model_1 (Malignant_train_images, Malignant_test_images):
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3,3), activation = "relu", padding='same', input_shape = (32,32,3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation = "relu"),
    #tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation = "relu"),
    #tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256,activation = "relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(80,activation = "relu"),
    tf.keras.layers.Dense(1,activation = "sigmoid")
])
    # training the model    
#opt = tf.keras.optimizers.Adam(learning_rate=0.01),
opt = keras.optimizers.Adam(learning_rate=0.001)
model.summary()

model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])

# checkpoint
#filepath="best_weights_p_neuron-{epoch:02d}-{val_acc:.2f}.hdf5"
filepath = r"/gdrive/MyDrive/5644_ML_Project/Models_1/cat_dogs_model_v1.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')
early_stop = EarlyStopping (monitor='val_acc', mode='max', verbose=0, patience=5)
callbacks_list = [early_stop,checkpoint]

history = model.fit_generator(train_generator, validation_data = validation_generator, steps_per_epoch = 750,epochs = 50, validation_steps = 375, verbose = 1, callbacks=callbacks_list) #

# serialize model to JSON
model_json = model.to_json()
with open("/gdrive/MyDrive/5644_ML_Project/Models_1/cat_dogs_model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("/gdrive/MyDrive/5644_ML_Project/Models_1/cat_dogs_model_v1.h5")
print("Saved model to disk")

loss , accuracy = model.evaluate(test_generator)

